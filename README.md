# python_labs

## –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 1

### –ó–∞–¥–∞–Ω–∏–µ 1
```python
name = input()
age = int(input())
print('–ò–º—è:' + name)
print('–í–æ–∑—Ä–∞—Å—Ç:' + str(age))
print("–ü—Ä–∏–≤–µ—Ç," + name + "!", " –ß–µ—Ä–µ–∑ –≥–æ–¥ —Ç–µ–±–µ –±—É–¥–µ—Ç " + str(age+1) + ".")
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 1](images/lab01/01.png)

### –ó–∞–¥–∞–Ω–∏–µ 2
```python
a = input()
b = float(input())
print("a: " + a.replace('.', ','))
print("b: " + str(b))
print("sum=" + f"{(float(a)+b):.2f}" + ";" + " avg=" + f"{(float(a)+b)/2:.2f}")
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 2](./images/lab01/02.png)

### –ó–∞–¥–∞–Ω–∏–µ 3
```python
price = int(input())
discount = int(input())
vat = int(input())

base = price * (1 - discount/100)
vat_amount = base * (vat/100)
total = base + vat_amount

print("–ë–∞–∑–∞ –ø–æ—Å–ª–µ —Å–∫–∏–¥–∫–∏: " + f"{base:.2f}" + " ‚ÇΩ")
print("–ù–î–°: " + f"{vat_amount:.2f}" + " ‚ÇΩ")
print("–ò—Ç–æ–≥–æ –∫ –æ–ø–ª–∞—Ç–µ: " + f"{total:.2f}" + " ‚ÇΩ")
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 3](./images/lab01/03.png)

### –ó–∞–¥–∞–Ω–∏–µ 4
```python
m = int(input())
print('–ú–∏–Ω—É—Ç—ã:' + ' ' + str(m))
print(str(m//60) + ":" + f"{(m%60):02d}")
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 4](./images/lab01/04.png)

### –ó–∞–¥–∞–Ω–∏–µ 5
```python
Surname = str(input())
Name = str(input())
Otchestvo = str(input())
print("–§–ò–û: ", Surname, Name, Otchestvo)
print("–ò–Ω–∏—Ü–∏–∞–ª—ã: ", Surname[0] + Name[0] + Otchestvo[0] + '.')
print("–î–ª–∏–Ω–∞ (—Å–∏–º–≤–æ–ª–æ–≤): " + str(len(Surname) + len(Name) + len(Otchestvo) + 2))
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 5](./images/lab01/05.png)

### –ó–∞–¥–∞–Ω–∏–µ 6
```python
form = []
tr = fl = 0
for n in range(int(input())):
    data = input().split()

    f = data[0] if len(data) > 0 else ""
    name = data[1] if len(data) > 1 else ""
    age = data[2] if len(data) > 2 else ""
    bol = data[3] if len(data) > 3 else ""

    form.append([f, name, age, bol])
for i in range(len(form)):
    print(str(i+1), form[i][0], form[i][1], form[i][2], form[i][3])
    if form[i][3] == 'True': tr += 1
    else: fl += 1
print("out:", tr, fl)
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 6](./images/lab01/06.png)

### –ó–∞–¥–∞–Ω–∏–µ 7
```python
a = input()
print("in: " + a)
c = []
for i in range(len(a)):
    if a[i].isupper():
        cnt = i
        break
c.append(''.join([a[i] for i in range(cnt, len(a), 3)]))
print("out:", *c)
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 7](./images/lab01/07.png)




## –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 2

### –ó–∞–¥–∞–Ω–∏–µ arrays.py
```python
def min_max(nums: list[float | int]) -> tuple[float | int, float | int]:
    try:
        return tuple([min(nums), max(nums)])
    except ValueError:
        return 'ValueError'


print('----------min_max----------')
print(min_max([1, 2, 3, 4, 5]))
print(min_max([42]))
print(min_max([-5, -2, -9]))
print(min_max([]))
print(min_max([1.5, 2, 2.0, -3.1]))

def unique_sorted(nums: list[float | int]) -> list[float | int]:
    a = sorted(list(set(nums)))
    return a

print('----------unique_sorted----------')
print(unique_sorted([3, 1, 2, 1, 3]))
print(unique_sorted([]))
print(unique_sorted([-1, -1, 0, 2, 2]))
print(unique_sorted([1.0, 1, 2.5, 2.5, 0]))

def flatten(mat: list[list | tuple]) -> list:
    rlist = list()
    for i in range(len(mat)):
        if isinstance(mat[i], list) or isinstance(mat[i], tuple):
            for j in mat[i]:
                rlist.append(j)
        else:
            return 'TypeError'
    return rlist



print('----------flatten----------')
print(flatten([[1, 2], [3, 4]]))
print(flatten(([1, 2], (3, 4, 5))))
print(flatten([[1], [], [2, 3]]))
print(flatten([[1, 2], "ab"]))
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 1](images/lab02/01.png)

### –ó–∞–¥–∞–Ω–∏–µ matrix.py
```python
def transpose(mat: list[list[float | int]]) -> list[list]:
    if not mat:
        return []

    row_len = len(mat[0])
    for row in mat:
        if len(row) != row_len:
            return 'ValueError'

    return [[mat[a][b] for a in range(len(mat))] for b in range(row_len)]


print('----------transpose----------')
print(transpose([[1, 2], [3, 4]]))
print(transpose([[1], [2], [3]]))
print(transpose([[1, 2], [3, 4]]))
print(transpose([]))
print(transpose([[1, 2], [3]]))


def row_sums(mat: list[list[float | int]]) -> list[float]:
    if not mat:
        return []

    row_len = len(mat[0])
    for row in mat:
        if len(row) != row_len:
            return 'ValueError'

    return [sum(i) for i in mat]


print('----------row_sum----------')
print(row_sums([[1, 2, 3], [4, 5, 6]]))
print(row_sums([[-1, 1], [10, -10]]))
print(row_sums([[0, 0], [0, 0]]))
print(row_sums([[1, 2], [3]]))


def col_sums(mat: list[list[float | int]]) -> list[float]:
    if not mat:
        return []

    row_len = len(mat[0])
    for row in mat:
        if len(row) != row_len:
            return 'ValueError'

    mat = transpose(mat)

    return [sum(i) for i in mat]


print('----------col_sums----------')
print(col_sums([[1, 2, 3], [4, 5, 6]]))
print(col_sums([[-1, 1], [10, -10]]))
print(col_sums([[0, 0], [0, 0]]))
print(col_sums([[1, 2], [3]]))
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 2](./images/lab02/02.png)

### –ó–∞–¥–∞–Ω–∏–µ tuples.py
```python
def info(fio: str, group: str, gpa: float) -> tuple:
    if not isinstance(fio, str):
        raise TypeError("fio –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π")
    if not isinstance(group, str):
        raise TypeError("group –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π")
    if not isinstance(gpa, (float, int)):
        raise TypeError("gpa –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —á–∏—Å–ª–æ–º")

    return ((lambda x: f"{x[0].capitalize()} {x[1][0].upper()}.{'' + x[2][0].upper() + '.' if len(x) > 2 else ''}")(
        [a.capitalize() for a in fio.strip().split() if a]), group, f"{gpa:.2f}")


def format_record(rec: tuple[str, str, float]) -> str:
    fio, group, gpa = rec
    inf = info(fio, group, gpa)
    answer = ''
    for _ in inf:
        answer += str(_) + ', '
    return answer[:-2]


print('----------format_record----------')
print(format_record(("–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á", "BIVT-25", 4.6)))
print(format_record(("–ü–µ—Ç—Ä–æ–≤ –ü—ë—Ç—Ä", "IKBO-12", 5.0)))
print(format_record(("–ü–µ—Ç—Ä–æ–≤ –ü—ë—Ç—Ä –ü–µ—Ç—Ä–æ–≤–∏—á", "IKBO-12", 5.0)))
print(format_record(("  —Å–∏–¥–æ—Ä–æ–≤–∞  –∞–Ω–Ω–∞   —Å–µ—Ä–≥–µ–µ–≤–Ω–∞ ", "ABB-01", 3.999)))
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 3](./images/lab02/03.png)

## –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 3

### –ó–∞–¥–∞–Ω–∏–µ zadanie A.py
```python
import re
def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    if casefold:
        text = text.casefold()
    if yo2e:
        text = text.replace('—ë', '–µ')
        text = text.replace('–Å', '–ï')
        text = text.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
    while '   ' in text:
        text = text.replace('   ', ' ')
    return text.strip()


def tokenize(text: str) -> list[str]:
    text = text.replace('!', '')
    text = re.split(r'[^\w-]+', text)
    return text


def count_freq(tokens: list[str]) -> dict[str, int]:
    dic = {}
    unique = set(tokens)
    for _ in unique:
        dic[_] = tokens.count(_)
    return dict(sorted(dic.items(), key=lambda x: (-x[1], x[0])))


def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    return sorted(freq.items(), key=lambda x: x[1], reverse=True)[:n]


print('----------normalize----------')
print(normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"))
print(normalize("—ë–∂–∏–∫, –Å–ª–∫–∞"))
print(normalize("Hello\r\nWorld"))
print(normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "))
print('----------tokenize----------')
print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"))
print(tokenize("hello,world!!!"))
print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"))
print(tokenize("2025 –≥–æ–¥"))
print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))
print('----------count_freq + top_n----------')
print(count_freq(["a","b","a","c","b","a"]))
print(count_freq(["bb","aa","bb","aa","cc"]))
print(top_n({"a":3,"b":2,"c":1}, n=2))
print(top_n({"aa":2,"bb":2,"cc":1}, n=2))
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 1](images/lab03/01.png)

### –ó–∞–¥–∞–Ω–∏–µ text_stats.py
```python
import sys
from src.lib import normalize, count_freq, top_n
text = input() # "–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! –ü—Ä–∏–≤–µ—Ç!!!"
tokens = []
for word in normalize(text).split():
    clean_word = word.strip('.,!!!!?;:"()[]{}')
    if clean_word:
        tokens.append(clean_word)

total_words = len(tokens)
unique_words = len(count_freq(tokens))
top_words = top_n(count_freq(tokens), 5)
print(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {total_words}")
print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {unique_words}")
print("–¢–æ–ø-5:")
for word, count in top_words:
        print(f"{word}:{count}")
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 2](./images/lab03/02.png)

## –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 4

### –ó–∞–¥–∞–Ω–∏–µ A.py
```python
from pathlib import Path
import sys

current_dir = Path(__file__).parent# –ø–æ–ª—É—á–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Ç–µ–∫—É—â–µ–≥–æ —Ñ–∞–π–ª–∞
sys.path.append(str(current_dir))# –î–æ–±–∞–≤–ª—è–µ–º —ç—Ç—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å –ø–æ–∏—Å–∫–∞ –º–æ–¥—É–ª–µ–π

from src.lib import tokenize, normalize, top_n, count_freq
from io_txt_csv import read_text, write_csv

def main():
    current_file = Path(__file__)# –ø–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –∏—Å–ø–æ–ª–Ω—è–µ–º–æ–º—É —Ñ–∞–π–ª—É
    print(f"–¢–µ–∫—É—â–∏–π —Ñ–∞–π–ª: {current_file}")# –∫–æ–∏ –≤—ã–≤–æ–¥–∏–º –µ–≥–æ

    input_path = current_file.parent / 'data' / 'input.txt'# –ø—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É
    output_path = current_file.parent / 'data' / 'output.csv'# –ø—É—Ç—å —É –≤—ã—Ö–æ–¥–Ω–æ–º—É

    print(f"–ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É: {input_path}")
    print(f"–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É: {output_path}")
    # —Å–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏ —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
    input_path.parent.mkdir(parents=True, exist_ok=True)
    input_path.write_text("–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! –ü—Ä–∏–≤–µ—Ç!!!", encoding='utf-8')#–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç—É–¥–∞ —Ç–µ–∫—Å—Ç
    print(f"–°–æ–∑–¥–∞–Ω/–æ–±–Ω–æ–≤–ª–µ–Ω —Ñ–∞–π–ª: {input_path}")

    result = read_text(input_path)#—á–∏—Ç–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —á—Ç–µ–Ω–∏—è: {result}")

    normalized_text = normalize(result)#–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –µ–≥–æ
    print(f"–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {normalized_text}")

    tokens = tokenize(normalized_text)#—Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ
    print(f"–¢–æ–∫–µ–Ω—ã: {tokens}")

    frequencies = count_freq(tokens)#—Å—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã
    print(f"–ß–∞—Å—Ç–æ—Ç—ã: {frequencies}")

    word_counts = top_n(frequencies, n=len(frequencies))#—Å–æ—Ä—Ç–∏—Ä—É–µ–º —á–∞—Å—Ç–æ—Ç—ã –ø–æ —á–∞—Å—Ç–æ—Ç–µ
    print(f"–ü–æ–¥—Å—á–µ—Ç —Å–ª–æ–≤: {word_counts}")

    write_csv(word_counts, output_path, header=('word', 'count'))#–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ csv
    print(f"CSV —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: {output_path}")

if __name__ == "__main__":
    main()# –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–∏ –ø—Ä—è–º–æ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 1](images/lab04/01.png)

### –ó–∞–¥–∞–Ω–∏–µ B.py
```python
from pathlib import Path
from src.lib import tokenize, normalize, top_n, count_freq
from io_txt_csv import read_text, write_csv

current_file = Path(__file__)#—Å–æ–∑–¥–∞–µ–º –æ–±—å–µ–∫—Ç path —è–≤–ª—è—é—â–∏–π—Å—è —Ç–µ–∫—É—â–∏–º —Ñ–∞–π–ª–æ–º
input_path = current_file.parent.parent / "src/data/input_test.txt"#—Å—Ç—Ä–æ–∏–º –ø—É—Ç—å –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏ –¥–æ–±–∞–≤–ª—è–µ–º —Ç—É–¥–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
output_path = current_file.parent.parent / "src/data/output.csv"

print(f"–¢–µ–∫—É—â–∏–π —Ñ–∞–π–ª: {current_file}")
print(f"–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {input_path}")
print(f"–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {output_path}")

input_path.parent.mkdir(parents=True, exist_ok=True)#—Å–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞  –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
if input_path.exists():#–ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏
    input_path.unlink()
input_path.write_text("–ü—Ä–∏–≤–µ—Ç", encoding="utf-8")#—Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç—É–¥–∞ —Ç–µ–∫—Å—Ç
print(f"–°–æ–∑–¥–∞–Ω –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª: {input_path}")

text = read_text(input_path, "utf-8")#—á–∏—Ç–∞–µ–º —Ñ–∞–π–ª
print(f"–ü—Ä–æ—á–∏—Ç–∞–Ω–æ: '{text}' ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")

normalized_text = normalize(text)#–∫–æ–ª–¥—É–µ–º –Ω–∞—à–∏–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏
tokens = tokenize(normalized_text)
frequencies = count_freq(tokens)

print(f"–¢–æ–∫–µ–Ω—ã: {tokens}")
print(f"–ß–∞—Å—Ç–æ—Ç—ã: {frequencies}")

word_counts = top_n(frequencies, n=len(frequencies))#–ø–æ–ª—É—á–∞–µ–º –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —á–∞—Å—Ç–æ—Ç–µ —Å–ª–æ–≤–∞
write_csv([[word, count] for word, count in word_counts],#–ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–ª–æ–≤–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏
          output_path, header=('word', 'count'))#–≤—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∑–∞–ø–∏—Å–∏
print(f"CSV —Å–æ–∑–¥–∞–Ω: {output_path}")

print(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {sum(frequencies.values())}")
print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(frequencies)}")

print("–¢–æ–ø 5 —Å–ª–æ–≤:")
top_5 = top_n(frequencies, n=5)
for i, (word, count) in enumerate(top_5, 1):
    print(f"  {i}. '{word}': {count}") if top_5 else print("  –ù–µ—Ç —Å–ª–æ–≤")
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 2](./images/lab04/02.png)
